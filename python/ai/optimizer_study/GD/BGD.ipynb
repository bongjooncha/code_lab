{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>BGD(Batch Gradient Descent)</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "식은 y = 3x + 4 에서 약간의 노이즈를 추가\n",
    "\n",
    "random.rand(100,1): 0이상 1이하의 숫로 이루어진 100 x 1 배열 생성\n",
    "\n",
    "random.randn(100,1): 표준 정규분포에서 무작위로 선택된 100 x 1 배열 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가상의 데이터 생성\n",
    "np.random.seed(42)    \n",
    "\n",
    "X = 2 * np.random.rand(100, 1) \n",
    "y = 4 + 3 * X + np.random.randn(100, 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ones(100,1): 1로 채워진 100 x 1 배열 생성, c_[ ]는 c_배열 안의 두 배열을 더함. (100 x 2) 배열이 됨\n",
    "\n",
    "여기서 앞을 1로 체운 이유는 상수는 X와 같이 따로 대입해야 할 값이 없기 때문이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.74908024]\n",
      " [1.         1.90142861]\n",
      " [1.         1.46398788]\n",
      " [1.         1.19731697]\n",
      " [1.         0.31203728]\n",
      " [1.         0.31198904]\n",
      " [1.         0.11616722]\n",
      " [1.         1.73235229]\n",
      " [1.         1.20223002]\n",
      " [1.         1.41614516]\n",
      " [1.         0.04116899]\n",
      " [1.         1.9398197 ]\n",
      " [1.         1.66488528]\n",
      " [1.         0.42467822]\n",
      " [1.         0.36364993]\n",
      " [1.         0.36680902]\n",
      " [1.         0.60848449]\n",
      " [1.         1.04951286]\n",
      " [1.         0.86389004]\n",
      " [1.         0.58245828]\n",
      " [1.         1.22370579]\n",
      " [1.         0.27898772]\n",
      " [1.         0.5842893 ]\n",
      " [1.         0.73272369]\n",
      " [1.         0.91213997]\n",
      " [1.         1.57035192]\n",
      " [1.         0.39934756]\n",
      " [1.         1.02846888]\n",
      " [1.         1.18482914]\n",
      " [1.         0.09290083]\n",
      " [1.         1.2150897 ]\n",
      " [1.         0.34104825]\n",
      " [1.         0.13010319]\n",
      " [1.         1.89777107]\n",
      " [1.         1.93126407]\n",
      " [1.         1.6167947 ]\n",
      " [1.         0.60922754]\n",
      " [1.         0.19534423]\n",
      " [1.         1.36846605]\n",
      " [1.         0.88030499]\n",
      " [1.         0.24407647]\n",
      " [1.         0.99035382]\n",
      " [1.         0.06877704]\n",
      " [1.         1.8186408 ]\n",
      " [1.         0.51755996]\n",
      " [1.         1.32504457]\n",
      " [1.         0.62342215]\n",
      " [1.         1.04013604]\n",
      " [1.         1.09342056]\n",
      " [1.         0.36970891]\n",
      " [1.         1.93916926]\n",
      " [1.         1.55026565]\n",
      " [1.         1.87899788]\n",
      " [1.         1.7896547 ]\n",
      " [1.         1.19579996]\n",
      " [1.         1.84374847]\n",
      " [1.         0.176985  ]\n",
      " [1.         0.39196572]\n",
      " [1.         0.09045458]\n",
      " [1.         0.65066066]\n",
      " [1.         0.77735458]\n",
      " [1.         0.54269806]\n",
      " [1.         1.65747502]\n",
      " [1.         0.71350665]\n",
      " [1.         0.56186902]\n",
      " [1.         1.08539217]\n",
      " [1.         0.28184845]\n",
      " [1.         1.60439396]\n",
      " [1.         0.14910129]\n",
      " [1.         1.97377387]\n",
      " [1.         1.54448954]\n",
      " [1.         0.39743136]\n",
      " [1.         0.01104423]\n",
      " [1.         1.63092286]\n",
      " [1.         1.41371469]\n",
      " [1.         1.45801434]\n",
      " [1.         1.54254069]\n",
      " [1.         0.1480893 ]\n",
      " [1.         0.71693146]\n",
      " [1.         0.23173812]\n",
      " [1.         1.72620685]\n",
      " [1.         1.24659625]\n",
      " [1.         0.66179605]\n",
      " [1.         0.1271167 ]\n",
      " [1.         0.62196464]\n",
      " [1.         0.65036664]\n",
      " [1.         1.45921236]\n",
      " [1.         1.27511494]\n",
      " [1.         1.77442549]\n",
      " [1.         0.94442985]\n",
      " [1.         0.23918849]\n",
      " [1.         1.42648957]\n",
      " [1.         1.5215701 ]\n",
      " [1.         1.1225544 ]\n",
      " [1.         1.54193436]\n",
      " [1.         0.98759119]\n",
      " [1.         1.04546566]\n",
      " [1.         0.85508204]\n",
      " [1.         0.05083825]\n",
      " [1.         0.21578285]]\n"
     ]
    }
   ],
   "source": [
    "X_b = np.c_[np.ones((100, 1)), X]\n",
    "print(X_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| lr | n_epochs | m |\n",
    "|-----|--------------|---|\n",
    "|학습률| 전체 데이터를 몇 번의 볼것인지 | 학습에 사용된 샘플 수|\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 초기값 설정\n",
    "lr = 0.1                  \n",
    "n_epochs = 1000        \n",
    "m = 100 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "theta_bgd는 bgd가 찾고자하는 변수이다. 현재 2개의 변수를 찾아야 하기에 랜덤한 숫자로 만들어지 2 x 1 배열을 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Gradient Descent (BGD) 구현\n",
    "theta_bgd = np.random.randn(2, 1)  \n",
    "theta_bgd_path = [] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![mse 함수](./img/mse_function.png)\n",
    "\n",
    " 이 코드에서 사용한 손실함수(loss funciton)은 MSE이다. MSE는 실제값과 예측치 theta를 넣고 계산한 값의 차를 제곱한 후 평균을 내는 손실함수이다.\n",
    "\n",
    "\n",
    "![편미분한 mse 함수](./img/mse_func_diff.png)\n",
    "\n",
    "MSE를 편미분하면 위와 같다. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_b.dot(theta_bgd)은 X_b와 theta_bgd를 행렬 곱한다. theta_bgd는 y = 3x + 4에서 3과 4의 추정치를 의미하고, X_b는 x와 상수 4에 곱할 1을 의미한다. 따라서 X_b.dot(theta_bgd)는 예측한 θ값 에서의 y 값을 의미한다.\n",
    "\n",
    "<strong>gradients = 2/m * X_b.T.dot(X_b.dot(theta_bgd) - y)</strong>에서 (X_b.dot(theta_bgd) - y)는 실제값과 예측치의 차이 값,<br/><br/>\n",
    "X_b.T.dot(X_b.dot(theta_bgd) - y)는 위에서 구한 차이 값에 편미분으로 인해 남겨진 θ값, X_b를 통해 X_b.T를 통해 전치 시킨 이후 곱한다. 각 θ에 대한 기울기 값을 구할수 있다.\n",
    "\n",
    "\n",
    "<br/><br/><br/><br/>\n",
    "gradient는 편미분 한 MSE의 값을 의미한다.\n",
    "\n",
    "<strong>theta_bgd = theta_bgd - eta * gradients</strong>는 이전에 예측한 θ값에서 위에서 구한 gradient(기울기)*eta(학습률)을 빼 오차율이 적은 θ값을 구한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1.10103284]\n",
      "  [2.5689581 ]]\n",
      "\n",
      " [[1.76167724]\n",
      "  [3.20430202]]\n",
      "\n",
      " [[2.17070217]\n",
      "  [3.55850018]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[4.21509616]\n",
      "  [2.77011339]]\n",
      "\n",
      " [[4.21509616]\n",
      "  [2.77011339]]\n",
      "\n",
      " [[4.21509616]\n",
      "  [2.77011339]]]\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(n_epochs):  # 1회 반복 당 시행되는 과정\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(theta_bgd) - y)\n",
    "    theta_bgd = theta_bgd - lr * gradients\n",
    "    theta_bgd_path.append(theta_bgd)\n",
    "    \n",
    "theta_bgd_path = np.array(theta_bgd_path)\n",
    "print(theta_bgd_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
